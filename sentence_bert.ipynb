{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/UKPLab/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T19:38:21.001025Z",
     "start_time": "2020-05-15T19:37:15.923416Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "from sklearn.metrics.classification import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.evaluation import BinaryEmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "columns:\n",
    "- txt_1\n",
    "- txt_2\n",
    "- label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T22:32:17.492940Z",
     "start_time": "2020-06-24T22:32:15.937803Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'data.csv'\n",
    "data_pd = pd.read_csv(file_name, index_col=0)\n",
    "print(data_pd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence encoding using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T20:27:36.538721Z",
     "start_time": "2020-05-15T20:26:46.033677Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:08:13.928106Z",
     "start_time": "2020-05-15T22:08:12.998652Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_1 = data_pd.txt_1.values.tolist()\n",
    "sent_2 = data_pd.txt_2.values.tolist()\n",
    "\n",
    "# obtain sentence embeddings\n",
    "sent_1_emb = model.encode(sent_1)\n",
    "sent_2_emb = model.encode(sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:11:46.065849Z",
     "start_time": "2020-05-15T22:11:46.048424Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_scores = 1 - (paired_cosine_distances(sent_1_emb, sent_2_emb))\n",
    "data_pd['cosine_distance'] = np.array(cosine_scores)\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T16:23:36.263204Z",
     "start_time": "2020-05-15T16:23:36.259162Z"
    }
   },
   "source": [
    "# fine tune model with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:13:54.473464Z",
     "start_time": "2020-05-15T22:13:54.454244Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pd, valid_pd, _, _ = train_test_split(data_pd, data_pd['label'],\n",
    "                                            test_size=0.4, random_state=42)\n",
    "dev_pd, test_pd, _, _ = train_test_split(valid_pd, valid_pd['label'],\n",
    "                                         test_size=0.5, random_state=42)\n",
    "\n",
    "print(train_pd[train_pd['class']==1].shape[0]/train_pd.shape[0], train_pd.shape[0])\n",
    "print(dev_pd[dev_pd['class']==1].shape[0]/dev_pd.shape[0], dev_pd.shape[0])\n",
    "print(test_pd[test_pd['class']==1].shape[0]/test_pd.shape[0], test_pd.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pre-train weights and set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:15:21.631339Z",
     "start_time": "2020-05-15T22:15:19.380344Z"
    }
   },
   "outputs": [],
   "source": [
    "# set logging to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()],\n",
    "                   )\n",
    "\n",
    "# specify huggingface/transformers pre-trained model, e.g: bert-base-uncased, roberta-base, xlm-roberta-base\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "# mapping tokens to embeddings with pre-trained model\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=128)\n",
    "\n",
    "# apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T20:31:49.335581Z",
     "start_time": "2020-05-15T20:31:49.308284Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "class DataReader(object):\n",
    "    \"\"\"\n",
    "    Reads data\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_examples(self, filename, max_examples=0):\n",
    "        \n",
    "        s1 = self.dataset['txt_1'].values.tolist()\n",
    "        s2 = self.dataset['txt_2'].values.tolist()\n",
    "        labels = self.dataset['label'].values.tolist()\n",
    "\n",
    "        examples = []\n",
    "        id = 0\n",
    "        for sentence_a, sentence_b, label in zip(s1, s2, labels):\n",
    "            guid = \"%s-%d\" % (filename, id)\n",
    "            id += 1\n",
    "            examples.append(InputExample(guid=guid, texts=[sentence_a, sentence_b], label=label))\n",
    "\n",
    "            if 0 < max_examples <= len(examples):\n",
    "                break\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:19:12.284248Z",
     "start_time": "2020-05-15T22:18:49.490637Z"
    }
   },
   "outputs": [],
   "source": [
    "# read dataset with customized reader \n",
    "batch_size = 16\n",
    "train_data_reader = DataReader(train_pd)\n",
    "dev_data_reader = DataReader(dev_pd)\n",
    "train_num_labels = 2\n",
    "\n",
    "# convert dataset to a DataLoader ready for training\n",
    "logging.info(\"Read train dataset\")\n",
    "train_data = SentencesDataset(train_data_reader.get_examples('train'), model=model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model=model, \n",
    "                                sentence_embedding_dimension=model.get_sentence_embedding_dimension(), \n",
    "                                num_labels=train_num_labels)\n",
    "\n",
    "logging.info(\"Read dev dataset\")\n",
    "dev_data = SentencesDataset(dev_data_reader.get_examples('dev'), model=model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = BinaryEmbeddingSimilarityEvaluator(dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T21:55:00.124448Z",
     "start_time": "2020-05-15T21:54:59.497921Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up model save path\n",
    "model_save_path = 'output/' + model_name.replace(\"/\", \"-\")\n",
    "if os.path.exists(model_save_path):\n",
    "        shutil.rmtree(model_save_path)\n",
    "        \n",
    "print('model_save_path:', model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T21:57:32.808147Z",
     "start_time": "2020-05-15T21:55:01.573891Z"
    }
   },
   "outputs": [],
   "source": [
    "# configure the training\n",
    "num_epochs = 1\n",
    "\n",
    "# 10% of train data for warm-up\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs / batch_size * 0.1) \n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# train model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:20:54.104345Z",
     "start_time": "2020-05-15T22:20:19.967574Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = model_save_path\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "test_data_reader = DataReader(test_pd)\n",
    "test_data = SentencesDataset(test_data_reader.get_examples('test'), model=model)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = BinaryEmbeddingSimilarityEvaluator(test_dataloader)\n",
    "\n",
    "model.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:21:54.663442Z",
     "start_time": "2020-05-15T22:21:23.710623Z"
    }
   },
   "outputs": [],
   "source": [
    "txt_1 = test_pd.txt_1.values.tolist()\n",
    "txt_2 = test_pd.txt_2.values.tolist()\n",
    "\n",
    "embed_1 = model.encode(txt_1)\n",
    "embed_2 = model.encode(txt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:21:55.973902Z",
     "start_time": "2020-05-15T22:21:55.965253Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_scores = 1 - (paired_cosine_distances(embed_1, embed_2))\n",
    "cosine_middle = np.median(cosine_scores)\n",
    "cosine_middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T22:22:04.530638Z",
     "start_time": "2020-05-15T22:22:04.519277Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test = test_pd['class']\n",
    "y_test_pred_classes = [1 if y>0.5 else 0 for y in cosine_scores]\n",
    "\n",
    "print('accuracy:', accuracy_score(y_test, y_test_pred_classes))\n",
    "precision_recall_fscore_support(y_test, y_test_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
